{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13eb7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#from bs4 import soupStrainer\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fe1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathp60s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=Pre-1960s'\n",
    "path60s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1960s'\n",
    "path70s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1970s'\n",
    "path80s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1980s'\n",
    "path90s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=1990s'\n",
    "path00s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2000s'\n",
    "path10s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2010s'\n",
    "path20s = 'https://charleyproject.org/case-searches/chronological-cases?chronology=2020s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78135583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "##results = soup.find_all('div', attrs={'class':'cases'})\n",
    "\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        href = a_tag.get('href') \n",
    "        if href and \"https://charleyproject.org/case/\" in href: \n",
    "            links.append(href)\n",
    "    \n",
    "#if contains 'https://charleyproject.org/case/ return to new df.\n",
    "##pre_1960s = pd.DataFrame(pre_1960s)\n",
    "    df = pd.DataFrame(links, columns=['Links:']) \n",
    "    df = df.dropna().reset_index(drop=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4e43eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_1960s = extract_links(pathp60s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b264f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Links:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://charleyproject.org/case/dorothy-harrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://charleyproject.org/case/robert-clarenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://charleyproject.org/case/charles-white-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://charleyproject.org/case/william-gaffney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://charleyproject.org/case/bessie-louise-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>https://charleyproject.org/case/alexander-walt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>https://charleyproject.org/case/grace-loretta-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>https://charleyproject.org/case/patrick-ayn-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>https://charleyproject.org/case/forrest-atmore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://charleyproject.org/case/iva-mildred-foss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Links:\n",
       "0    https://charleyproject.org/case/dorothy-harrie...\n",
       "1    https://charleyproject.org/case/robert-clarenc...\n",
       "2    https://charleyproject.org/case/charles-white-...\n",
       "3      https://charleyproject.org/case/william-gaffney\n",
       "4    https://charleyproject.org/case/bessie-louise-...\n",
       "..                                                 ...\n",
       "129  https://charleyproject.org/case/alexander-walt...\n",
       "130  https://charleyproject.org/case/grace-loretta-...\n",
       "131  https://charleyproject.org/case/patrick-ayn-st...\n",
       "132  https://charleyproject.org/case/forrest-atmore...\n",
       "133   https://charleyproject.org/case/iva-mildred-foss\n",
       "\n",
       "[134 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_1960s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_1960s = extract_links(pathp60s)\n",
    "x_1960s = extract_links(path60s)\n",
    "x_1970s = extract_links(path70s)\n",
    "x_1980s = extract_links(path80s)\n",
    "x_1990s = extract_links(path90s)\n",
    "x_2000s = extract_links(path00s)\n",
    "x_2010s = extract_links(path10s)\n",
    "x_2020s = extract_links(path20s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_data = pd.concat([pre_1960s, x_1960s, x_1970s, x_1980s,\n",
    "                            x_1990s, x_2000s, x_2010s, x_2020s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_years_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c0d37",
   "metadata": {},
   "source": [
    "yearbook = ['Pre-1960s', '1960s', '1970s', '1980s', '1990s', '2000s', '2010s', '2020s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aff017",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_people_df = pd.DataFrame()\n",
    "#link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da2520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 134/134 [01:54<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Links:  \\\n",
      "0     https://charleyproject.org/case/dorothy-harrie...   \n",
      "1     https://charleyproject.org/case/dorothy-harrie...   \n",
      "2     https://charleyproject.org/case/dorothy-harrie...   \n",
      "3     https://charleyproject.org/case/dorothy-harrie...   \n",
      "4     https://charleyproject.org/case/dorothy-harrie...   \n",
      "...                                                 ...   \n",
      "1270   https://charleyproject.org/case/iva-mildred-foss   \n",
      "1271   https://charleyproject.org/case/iva-mildred-foss   \n",
      "1272   https://charleyproject.org/case/iva-mildred-foss   \n",
      "1273   https://charleyproject.org/case/iva-mildred-foss   \n",
      "1274   https://charleyproject.org/case/iva-mildred-foss   \n",
      "\n",
      "                                name Missing Since  \\\n",
      "0     Dorothy Harriet Camille Arnold    12/12/1910   \n",
      "1     Dorothy Harriet Camille Arnold    12/12/1910   \n",
      "2     Dorothy Harriet Camille Arnold    12/12/1910   \n",
      "3     Dorothy Harriet Camille Arnold    12/12/1910   \n",
      "4     Dorothy Harriet Camille Arnold    12/12/1910   \n",
      "...                              ...           ...   \n",
      "1270                Iva Mildred Foss    11/20/1960   \n",
      "1271                Iva Mildred Foss    11/20/1960   \n",
      "1272                Iva Mildred Foss    11/20/1960   \n",
      "1273                Iva Mildred Foss    11/20/1960   \n",
      "1274                Iva Mildred Foss    11/20/1960   \n",
      "\n",
      "                             Missing From      Classification     Sex   Race  \\\n",
      "0                                     NaN                 NaN     NaN    NaN   \n",
      "1     Manhattan, \\t\\t\\t\\t\\t\\t\\t\\tNew York                 NaN     NaN    NaN   \n",
      "2     Manhattan, \\t\\t\\t\\t\\t\\t\\t\\tNew York             Missing     NaN    NaN   \n",
      "3     Manhattan, \\t\\t\\t\\t\\t\\t\\t\\tNew York             Missing  Female    NaN   \n",
      "4     Manhattan, \\t\\t\\t\\t\\t\\t\\t\\tNew York             Missing  Female  White   \n",
      "...                                   ...                 ...     ...    ...   \n",
      "1270  Johnstown, \\t\\t\\t\\t\\t\\t\\t\\tNew York  Endangered Missing  Female  White   \n",
      "1271  Johnstown, \\t\\t\\t\\t\\t\\t\\t\\tNew York  Endangered Missing  Female  White   \n",
      "1272  Johnstown, \\t\\t\\t\\t\\t\\t\\t\\tNew York  Endangered Missing  Female  White   \n",
      "1273  Johnstown, \\t\\t\\t\\t\\t\\t\\t\\tNew York  Endangered Missing  Female  White   \n",
      "1274  Johnstown, \\t\\t\\t\\t\\t\\t\\t\\tNew York  Endangered Missing  Female  White   \n",
      "\n",
      "         Date of Birth           Age Height and Weight  \\\n",
      "0                  NaN           NaN               NaN   \n",
      "1                  NaN           NaN               NaN   \n",
      "2                  NaN           NaN               NaN   \n",
      "3                  NaN           NaN               NaN   \n",
      "4                  NaN           NaN               NaN   \n",
      "...                ...           ...               ...   \n",
      "1270  05/30/1901 (123)  59 years old               NaN   \n",
      "1271  05/30/1901 (123)  59 years old   5'2, 130 pounds   \n",
      "1272  05/30/1901 (123)  59 years old   5'2, 130 pounds   \n",
      "1273  05/30/1901 (123)  59 years old   5'2, 130 pounds   \n",
      "1274  05/30/1901 (123)  59 years old   5'2, 130 pounds   \n",
      "\n",
      "                           Clothing/Jewelry Description  \\\n",
      "0                                                   NaN   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                                   NaN   \n",
      "...                                                 ...   \n",
      "1270                                                NaN   \n",
      "1271                                                NaN   \n",
      "1272  A long gray coat. Carrying a blue train case, ...   \n",
      "1273  A long gray coat. Carrying a blue train case, ...   \n",
      "1274  A long gray coat. Carrying a blue train case, ...   \n",
      "\n",
      "                         Distinguishing Characteristics  \\\n",
      "0                                                   NaN   \n",
      "1                                                   NaN   \n",
      "2                                                   NaN   \n",
      "3                                                   NaN   \n",
      "4                                                   NaN   \n",
      "...                                                 ...   \n",
      "1270                                                NaN   \n",
      "1271                                                NaN   \n",
      "1272                                                NaN   \n",
      "1273                                                NaN   \n",
      "1274  Caucasian female. Brown hair. Foss may wear pr...   \n",
      "\n",
      "                 Associated Vehicle(s) Medical Conditions  \n",
      "0                                  NaN                NaN  \n",
      "1                                  NaN                NaN  \n",
      "2                                  NaN                NaN  \n",
      "3                                  NaN                NaN  \n",
      "4                                  NaN                NaN  \n",
      "...                                ...                ...  \n",
      "1270                               NaN                NaN  \n",
      "1271                               NaN                NaN  \n",
      "1272                               NaN                NaN  \n",
      "1273  Green Ford sedan (accounted for)                NaN  \n",
      "1274  Green Ford sedan (accounted for)                NaN  \n",
      "\n",
      "[1275 rows x 14 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store all data \n",
    "missing_people_df = pd.DataFrame() \n",
    "# Iterate over each link in all_years_data \n",
    "for i in tqdm(range(pre_1960s.shape[0])): \n",
    "    link_str = pre_1960s.at[i, 'Links:'] \n",
    "    r2 = requests.get(link_str) \n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser') \n",
    "    # Extract data \n",
    "    person = { \"Links:\": [link_str], \n",
    "              \"name\": [soup2.find('h1').get_text()] \n",
    "             } \n",
    "    for strong_tag in soup2.find_all('strong'): \n",
    "        k = strong_tag.text.strip() \n",
    "        v = strong_tag.next_sibling.strip() \n",
    "        person[k] = [v] \n",
    "        # Convert to DataFrame \n",
    "        persondf = pd.DataFrame(person) \n",
    "        # Merge with the main DataFrame \n",
    "        if not missing_people_df.empty: \n",
    "            missing_people_df = pd.concat([missing_people_df, persondf], ignore_index=True) \n",
    "        else: \n",
    "            missing_people_df = persondf \n",
    "# Display the final DataFrame \n",
    "print(missing_people_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3290e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "missing_people_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each link in all_years_data\n",
    "for i in tqdm(range(pre_1960s.shape[0])):\n",
    "    link_str = pre_1960s.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    \n",
    "    # Extract data\n",
    "    person = {\n",
    "        \"Links:\": [link_str],\n",
    "        \"name\": [soup2.find('h1').get_text()]\n",
    "    }\n",
    "    \n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        k = strong_tag.text.strip()\n",
    "        v = strong_tag.next_sibling.strip()\n",
    "        person[k] = [v]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    persondf = pd.DataFrame(person)\n",
    "    \n",
    "    # Merge with the main DataFrame\n",
    "    if not missing_people_df.empty:\n",
    "        missing_people_df = pd.concat([missing_people_df, persondf], ignore_index=True)\n",
    "    else:\n",
    "        missing_people_df = persondf\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(missing_people_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778963c",
   "metadata": {},
   "source": [
    "for i in range(0, (all_years_data.size-1)):\n",
    "    link_str = all_years_data.at[i, 'Links:']\n",
    "    r2 = requests.get(link_str)\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "\n",
    "    person = {\"Links:\": [link_str],\n",
    "            \"name\": [soup2.find('h1').get_text()],}\n",
    "\n",
    "    #for h_tag in soup2.find('h1'):\n",
    "    #    dor = (h_tag.get_text())\n",
    "    #    name.append(dor)\n",
    "        \n",
    "    i = 0\n",
    "\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        k = strong_tag.text.strip()\n",
    "        v = strong_tag.next_sibling.strip()\n",
    "        #var.append(strong_tag.text.strip() + \":\")\n",
    "        #info.append(strong_tag.next_sibling.strip())\n",
    "        person[k] = [v]\n",
    "        i+=1\n",
    "\n",
    "    persondf = pd.DataFrame(person)\n",
    "\n",
    "    if not missing_people_df.empty:\n",
    "        missing_people_df = pd.merge(missing_people_df, persondf, how='outer')\n",
    "    else:\n",
    "        missing_people_df = persondf\n",
    "\n",
    "missing_people_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c529373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_people_df.to_excel('2020s.xlsx')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c76abf",
   "metadata": {},
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a710c26",
   "metadata": {},
   "source": [
    "persondf = pd.DataFrame(person)\n",
    "persondf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f377cae",
   "metadata": {},
   "source": [
    "#missing_people_df = pd.merge(missing_people_df, persondf, how='outer')\n",
    "if not missing_people_df.empty:\n",
    "    missing_people_df = pd.merge(missing_people_df, persondf, how='outer')\n",
    "else:\n",
    "    missing_people_df = persondf\n",
    "\n",
    "missing_people_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f080c8",
   "metadata": {},
   "source": [
    "combo = {}\n",
    "for key in var: \n",
    "    for value in info:\n",
    "        combo[key] = value\n",
    "        info.remove(value) \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ecec35",
   "metadata": {},
   "source": [
    "combo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681d87e",
   "metadata": {},
   "source": [
    "r=requests.get(path + yearbook[0])\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "results = soup.find_all('div', attrs={'class':'cases'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237fcb9",
   "metadata": {},
   "source": [
    "r_test = requests.get('https://charleyproject.org/case/dorothy-harriet-camille-arnold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86086b",
   "metadata": {},
   "source": [
    "soup_test = BeautifulSoup(r_test.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449724f8",
   "metadata": {},
   "source": [
    "soup_test = soup_test.find_all('strong')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a2d30",
   "metadata": {},
   "source": [
    "for strong_tag in soup_test.find_all('strong'):\n",
    "    print(strong_tag.next_sibling.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5142e8",
   "metadata": {},
   "source": [
    "paragraph = soup_test.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356002b",
   "metadata": {},
   "source": [
    "paragraph = soup_test.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7deebb9",
   "metadata": {},
   "source": [
    "name_key = {'Name': name}\n",
    "combo.update(name_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222865ba",
   "metadata": {},
   "source": [
    "i=0\n",
    "while i < len(link):\n",
    "    r2 = requests.get(link.at[i, 'Links:'])\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    for strong_tag in soup2.find_all('strong'):\n",
    "        link.at[i, 'Missing Since:'] = strong_tag.next_sibling.strip()\n",
    "        link.at[i, 'Location:'] = strong_tag.next_sibling.strip()\n",
    "        \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b05c35",
   "metadata": {},
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce186a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
